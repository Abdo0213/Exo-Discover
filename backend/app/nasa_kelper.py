# -*- coding: utf-8 -*-
"""nasa_kelper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10NmbSLTTCNQAehrNO6nDcbdf6VEpdzO4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.simplefilter(action='ignore', category=RuntimeWarning)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


from sklearn.metrics import classification_report
import pandas as pd
import joblib
import google.generativeai as genai
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import os
import pandas as pd
import joblib
import google.generativeai as genai

# Set Gemini API key
genai.configure(api_key="AIzaSyCxAPrut0iWskvGYA9deOw3QvLO18N43Og")

# Load Gemini model
#gemini = genai.GenerativeModel("gemini-1.5-flash")  # you can also use gemini-pro
gemini = genai.GenerativeModel("models/gemini-2.5-pro")

# IMPORTANT: ensure this order matches the order used when training the RF
columns_order = [
    'koi_fpflag_nt','koi_fpflag_ss','koi_fpflag_co','koi_fpflag_ec',
    'koi_period','koi_period_err2','koi_time0bk','koi_impact','koi_duration',
    'koi_depth','koi_prad','koi_teq','koi_insol','koi_model_snr',
    'koi_tce_plnt_num','koi_steff','koi_slogg','koi_srad','ra','dec','koi_kepmag'
]

fi = getattr(rf, "feature_importances_", None)
if fi is None:
    raise ValueError("Loaded Random Forest has no feature_importances_.")
min_len = min(len(columns_order), len(fi))
feature_importances = {columns_order[i]: float(fi[i]) for i in range(min_len)}
sorted_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True)
# Keep top 5 for context
top_n = 5
top_features = sorted_importances[:top_n]
print("Top features:", top_features)

# Cell 3
def top_features_text(sorted_importances, top_n=3):
    top = sorted_importances[:top_n]
    return "\n".join([f"{i+1}. {name} (importance={imp:.4f})"
                      for i, (name, imp) in enumerate(top)])

# Cell 4 - updated to force Gemini to show the big contributing columns and why
def chatbot_predict_model_reason(features_dict, top_n=3):
    """
    Predict with RF and ask Gemini to produce a short, focused explanation of WHY the ML model chose its output,
    emphasizing the top_n features that most affected the decision and giving a one-sentence reason for each.
    Output format required:
      1) Ranked list of top_n features (name + importance).
      2) For each listed feature: one short sentence explaining how this feature's value influenced the prediction.
      3) Final 2-3 bullet summary why the model chose the label.
    """
    # Predict
    df = pd.DataFrame([features_dict])
    pred_class = rf.predict(df)[0]
    pred_label = "CANDIDATE" if pred_class == 1 else "CONFIRMED"

    # Build features block
    feature_lines = "\n".join([f"- {k}: {v}" for k, v in features_dict.items()])

    # Top features (pre-computed sorted_importances from Cell 2)
    top_block = top_features_text(sorted_importances, top_n=top_n)

    # Include the top feature values for this sample
    top_values_lines = []
    for name, imp in sorted_importances[:top_n]:
        val = features_dict.get(name, "N/A")
        top_values_lines.append(f"- {name}: {val} (importance={imp:.4f})")
    top_values_block = "\n".join(top_values_lines)

    # New prompt: ask explicitly to list top features and give one-sentence cause for each
    prompt = f"""
You are a concise, friendly AI assistant that explains WHY a machine learning model (Random Forest) produced a prediction.
Keep the answer short and highly focused. Use bullet points only.

Context:
- Model type: Random Forest classifier
- Model prediction for this sample: {pred_label}
- Precomputed top {top_n} features by RF importance (ranked):
{top_block}

- Values of those top features for this sample:
{top_values_block}

Task (strict requirements):
1) First, produce a ranked list of the top {top_n} features (one bullet per feature). For each feature bullet include:
   - Feature name and importance (as shown above).
   - ONE short sentence (maximum 20 words) that explains HOW the current value of that feature contributed to the RF decision (e.g., "High X raises probability because ...", or "Low Y suggests noise, reducing probability").

2) After the ranked feature bullets, provide a final compact summary of **2–3 bullet points** that synthesizes why the model labeled this sample {pred_label} (refer to the top features above in the summary).

Formatting rules (must follow exactly):
- Use only bullet points (no paragraphs).
- Each top-feature item must be exactly one bullet and one short explanatory sentence.
- Summary must be 2 or 3 bullets only.
- Do NOT list or explain other columns.
- Do NOT include code, model internals, or long text.

Here are the sample features again (for reference):
{feature_lines}
"""

    # Call Gemini (positional argument)
    try:
        response = gemini.generate_content(prompt)
    except Exception as e:
        raise RuntimeError(f"Gemini API call failed: {e}")

    return response.text

# Cell 1
import os
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import google.generativeai as genai

# Load API key from environment variable
api_key = os.getenv("models/gemini-2.5-pro")

genai.configure(api_key=api_key)

# # Choose a Gemini model (fast + cost-efficient)
# gemini = genai.GenerativeModel("models/gemini-2.5-flash")
# print("Gemini configured:", "models/gemini-2.5-flash")

# Cell 5
def build_explain_prompt(pred_label, top_features, sample_values, feature_name_map=None, top_n=3):
    """
    top_features: list of (name, importance) sorted desc
    sample_values: dict with feature->value for the sample
    """
    lines = []
    for i, (name, imp) in enumerate(top_features[:top_n]):
        rname = pretty_name(name, feature_name_map)
        val = sample_values.get(name, "N/A")
        lines.append(f"{i+1}. {rname} (importance={imp:.4f}) — value: {val}")
    top_block = "\n".join(lines)

    prompt = f"""
You are a concise, friendly assistant that explains WHY a trained Random Forest produced a prediction.
Context:
- Prediction for this sample: {pred_label}
- Top {top_n} features (ranked, human-readable) and their values:
{top_block}

Task (strict):
1) For each top feature, produce exactly ONE bullet (<=20 words) explaining how this feature's value influenced the RF decision (e.g., "High X increases likelihood because ...").
2) Then produce exactly TWO summary bullets explaining overall why the model chose {pred_label}, referencing the top features.
Constraints:
- Use only bullet points.
- Do NOT explain other columns.
- Do NOT include code or model internals.
- Total bullets = top_n + 2.

Answer now with the requested bullets.
"""
    return prompt

def call_gemini(prompt):
    try:
        res = gemini.generate_content(prompt)  # positional argument
        return res.text
    except Exception as e:
        raise RuntimeError(f"Gemini API call failed: {e}")

# Cell 6
def explain_model_via_chatbot(source, target_column=None, model_path=None, feature_name_map=None, top_n=3, train_if_missing=True):
    """
    - source: CSV path (str) OR pandas.DataFrame OR dict (single sample).
    - If source is dataset (DataFrame or CSV with >1 row) and no existing model, it will train RF (if train_if_missing True).
    - If source is a single-sample dict, you MUST provide model_path pointing to a saved RF model, or set train_if_missing and provide a dataset instead.
    - Returns Gemini's explanation text.
    """
    # Load input
    loaded = load_data(source)
    # Case 1: dataset (DataFrame)
    if isinstance(loaded, pd.DataFrame):
        df = loaded
        # Train or load model using df
        rf_model, feature_columns, sorted_importances = train_or_load_rf(df, target_column=target_column, model_path=model_path, train_if_missing=train_if_missing)
        # Default sample: last row (excluding target)
        tgt = infer_target_column(df, target_column) if target_column or len(df.columns)>1 else None
        sample = df.drop(columns=[tgt]).iloc[-1].to_dict() if tgt else df.iloc[-1].to_dict()
    else:
        # loaded is dict: single sample -> need model_path exists
        sample = loaded
        if not model_path or not os.path.exists(model_path):
            raise ValueError("For single-sample input provide a valid model_path to a saved RF model.")
        rf_model = joblib.load(model_path)
        # feature columns from model if available
        if hasattr(rf_model, "feature_names_in_"):
            feature_columns = list(rf_model.feature_names_in_)
        else:
            # fallback: keys of sample
            feature_columns = list(sample.keys())
        # Attempt to build sorted_importances using rf_model (defensive)
        fi = getattr(rf_model, "feature_importances_", None)
        if fi is None or len(fi) != len(feature_columns):
            sorted_importances = [(c, 0.0) for c in feature_columns]
        else:
            sorted_importances = sorted({feature_columns[i]: float(fi[i]) for i in range(len(feature_columns))}.items(), key=lambda x: x[1], reverse=True)

    # Predict sample label
    Xs = pd.DataFrame([sample])[ [c for c in feature_columns if c in sample] ]
    pred_class = rf_model.predict(Xs)[0]
    pred_label = "CANDIDATE" if pred_class == 1 else "CONFIRMED"

    # Build prompt with top features and their sample values
    explanation_prompt = build_explain_prompt(pred_label, sorted_importances, sample, feature_name_map=feature_name_map, top_n=top_n)

    # Call Gemini and return text
    explanation = call_gemini(explanation_prompt)
    return explanation

#####     chat bot in space

# Set Gemini API key
genai.configure(api_key="AIzaSyCxAPrut0iWskvGYA9deOw3QvLO18N43Og")

# Load Gemini model
#gemini = genai.GenerativeModel("gemini-1.5-flash")  # you can also use gemini-pro
gemini = genai.GenerativeModel("models/gemini-2.5-pro")

# ======================================
# Cell 0 - Install SDK (run once)
# ======================================
# %pip install --upgrade --quiet google-genai

# ======================================
# Cell 1 - Setup API key
# ======================================
import os
from google import genai

# Set your Gemini API key
os.environ["GOOGLE_API_KEY"] = "AIzaSyCxAPrut0iWskvGYA9deOw3QvLO18N43Og"   # <-- replace with your key

# Init client
client = genai.Client(api_key=os.environ["GOOGLE_API_KEY"])

# ======================================
# Cell 2 - Define prompt (system rules)
# ======================================
prompt_chatbot = """
You are an assistant model created by the ExoDiscovery team. Follow these rules exactly:

1) TOPIC SCOPE:
   - You MUST ONLY answer questions directly about space, including but not limited to:
     astronomy, astrophysics, space physics, planetary science, orbital mechanics, space chemistry,
     space materials science, spacecraft engineering concepts (conceptual), exoplanets, stellar evolution,
     atmospheric escape, radiation environments in space, habitability basics, and NASA/ESA/JAXA mission facts.
   - If a user asks about anything outside of space, reply exactly:
       "I’m a model created by the ExoDiscovery team and I cannot answer that."

2) IDENTITY & STYLE:
   - Start every valid answer with: "As a model created by the ExoDiscovery team:"
   - Be concise, factual, and avoid speculation.

3) CITATIONS:
   - Do not fabricate. You may recommend NASA, ESA, JAXA, or arXiv sources.

4) SAFETY:
   - No harmful instructions. If asked, respond with the refusal line above.
"""

# ======================================
# Cell 3 - Example user questions
# ======================================
user_message_1 = "What causes the aurora on Earth, and how does it differ on Jupiter?"
user_message_2 = (
    "I have a dataset for classifying exoplanet habitability. "
    "Columns: orbital_period, planetary_radius, equilibrium_temperature, "
    "stellar_insolation, eccentricity, host_star_temperature. "
    "Show the big contributing columns and why they matter."
)
non_space_test = "Which stock should I buy right now?"

# ======================================
# Cell 4 - Ask Gemini (Aurora example)
# ======================================
model_name = genai.GenerativeModel("models/gemini-2.5-pro")
  # You can also try gemini-1.5-flash

content_aurora = prompt_chatbot + "\n\nUser question:\n" + user_message_1

resp1 = client.models.generate_content(
    model=model_name,
    contents=[content_aurora],
    config={"temperature": 0.0, "max_output_tokens": 512}
)

print("=== MODEL RESPONSE (Aurora) ===")
print(resp1.text)

# ======================================
# Cell 5 - Ask Gemini (Feature importance example)
# ======================================
content_feat = prompt_chatbot + "\n\nUser question:\n" + user_message_2

resp2 = client.models.generate_content(
    model=model_name,
    contents=[content_feat],
    config={"temperature": 0.0, "max_output_tokens": 512}
)

print("\n=== MODEL RESPONSE (Feature Importance) ===")
print(resp2.text)

# ======================================
# Cell 6 - Non-space test (should refuse)
# ======================================
content_nonspace = prompt_chatbot + "\n\nUser question:\n" + non_space_test

resp3 = client.models.generate_content(
    model=model_name,
    contents=[content_nonspace],
    config={"temperature": 0.0, "max_output_tokens": 120}
)

print("\n=== MODEL RESPONSE (Non-space test) ===")
print(resp3.text)

# ======================================
# Cell 0 - Install SDK (run once)
# ======================================
# %pip install --upgrade --quiet google-genai

# ======================================
# Cell 1 - Setup API key
# ======================================
import os
import google.generativeai as genai

# Set your Gemini API key
# It's recommended to use Colab Secrets for API keys
# from google.colab import userdata
# os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')

# For direct API key use (less secure):
os.environ["GOOGLE_API_KEY"] = "AIzaSyCxAPrut0iWskvGYA9deOw3QvLO18N43Og"   # <-- replace with your key or use Secrets


genai.configure(api_key=os.environ["GOOGLE_API_KEY"]) # Use genai.configure for API key based auth

# List available models to find one that supports generate_content
# print("Available models:")
# for m in genai.list_models():
#     if 'generateContent' in m.supported_generation_methods:
#         print(m.name)

# ======================================
# Cell 2 - Define prompt (system rules)
# ======================================
prompt_chatbot = """
You are an assistant model created by the ExoDiscovery team. Follow these rules exactly:

1) TOPIC SCOPE:
   - You MUST ONLY answer questions directly about space, including but not limited to:
     astronomy, astrophysics, space physics, planetary science, orbital mechanics, space chemistry,
     space materials science, spacecraft engineering concepts (conceptual), exoplanets, stellar evolution,
     atmospheric escape, radiation environments in space, habitability basics, and NASA/ESA/JAXA mission facts.
   - If a user asks about anything outside of space, reply exactly:
       "I’m a model created by the ExoDiscovery team and I cannot answer that."

2) IDENTITY & STYLE:
   - Start every valid answer with: "As a model created by the ExoDiscovery team:"
   - Be concise, factual, and avoid speculation.

3) CITATIONS:
   - Do not fabricate. You may recommend NASA, ESA, JAXA or arXiv sources.

4) SAFETY:
   - No harmful instructions. If asked, respond with the refusal line above.
"""

# ======================================
# Cell 3 - Example user questions
# ======================================
user_message_1 = "What causes the aurora on Earth, and how does it differ on Jupiter?"
user_message_2 = (
    "I have a dataset for classifying exoplanet habitability. "
    "Columns: orbital_period, planetary_radius, equilibrium_temperature, "
    "stellar_insolation, eccentricity, host_star_temperature. "
    "Show the big contributing columns and why they matter."
)
non_space_test = "Which stock should I buy right now?"

# ======================================
# Cell 4 - Ask Gemini (Aurora example)
# ======================================
# Use GenerativeModel directly
# Replace with a model name from the list above that supports generateContent
model = genai.GenerativeModel("models/gemini-2.5-flash") # Using an available model name

content_aurora = prompt_chatbot + "\n\nUser question:\n" + user_message_1

resp1 = model.generate_content(
    content_aurora, # Pass content directly
    generation_config={"temperature": 0.0, "max_output_tokens": 512}
)

print("=== MODEL RESPONSE (Aurora) ===")
print(resp1.text)

# ======================================
# Cell 5 - Ask Gemini (Feature importance example)
# ======================================
content_feat = prompt_chatbot + "\n\nUser question:\n" + user_message_2

resp2 = model.generate_content(
    content_feat, # Pass content directly
    generation_config={"temperature": 0.0, "max_output_tokens": 512}
)

print("\n=== MODEL RESPONSE (Feature Importance) ===")
print(resp2.text)

# ======================================
# Cell 6 - Non-space test (should refuse)
# ======================================
content_nonspace = prompt_chatbot + "\n\nUser question:\n" + non_space_test

resp3 = model.generate_content(
    content_nonspace, # Pass content directly
    generation_config={"temperature": 0.0, "max_output_tokens": 120}
)

print("\n=== MODEL RESPONSE (Non-space test) ===")
print(resp3.text)

